{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all libraries needed for this assigment\n",
    "\n",
    "from pandas import DataFrame, read_csv\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from os import path\n",
    "\n",
    "import string\n",
    "import re\n",
    "import sys \n",
    "import matplotlib \n",
    "%matplotlib inline\n",
    "import os\n",
    "import csv\n",
    "import glob\n",
    "import folium\n",
    "import string\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Pre-processing and cleaning the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions: These function are going to clean our data from all the unnecessary infos , like \"\\n\" or \"\\u0111\" and maybe some URLs.\n",
    "\n",
    "def Convert(string): \n",
    "    li = list(string.split(\" \")) \n",
    "    return li\n",
    "\n",
    "def sNormalization(string):\n",
    "\n",
    "    string = Convert(string)\n",
    "    \n",
    "    counter=0\n",
    "    removeList=[]\n",
    "\n",
    "    for elements in range(len(string)):\n",
    "        for letter in string[elements]:\n",
    "            if(letter=='\\\\'):\n",
    "                removeList.append(elements)\n",
    "                break\n",
    "    \n",
    "    \n",
    "    for index in range(len(removeList)):\n",
    "        string.pop(removeList[index]+counter)\n",
    "        counter= counter-1\n",
    "\n",
    "    \n",
    "    finalString=\"\"\n",
    "    for i in range(len(string)):\n",
    "        finalString=finalString+string[i]+\" \"\n",
    "\n",
    "    \n",
    "    cleanString = re.sub('\\W+',' ', finalString )\n",
    "\n",
    "    return cleanString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Insult</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20120603163526Z</td>\n",
       "      <td>like this if you are a tribe fan</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20120531215447Z</td>\n",
       "      <td>you re idiot</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20120823164228Z</td>\n",
       "      <td>i am a woman babs and the only war on women i...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20120826010752Z</td>\n",
       "      <td>wow you benefitted so many wins this year fro...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20120602223825Z</td>\n",
       "      <td>haha green me red you now loser whos winning ...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20120603202442Z</td>\n",
       "      <td>and god both the difference between a fag and ...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20120603163604Z</td>\n",
       "      <td>oh go kiss the ass of a goat and you dummycra...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20120602223902Z</td>\n",
       "      <td>not a chance kid you re wrong</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20120528064125Z</td>\n",
       "      <td>on some real shit fuck live jasmin</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20120603071243Z</td>\n",
       "      <td>ok but where the hell was it released you all...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  Insult             Date  \\\n",
       "0   1     NaN  20120603163526Z   \n",
       "1   2     NaN  20120531215447Z   \n",
       "2   3     NaN  20120823164228Z   \n",
       "3   4     NaN  20120826010752Z   \n",
       "4   5     NaN  20120602223825Z   \n",
       "5   6     NaN  20120603202442Z   \n",
       "6   7     NaN  20120603163604Z   \n",
       "7   8     NaN  20120602223902Z   \n",
       "8   9     NaN  20120528064125Z   \n",
       "9  10     NaN  20120603071243Z   \n",
       "\n",
       "                                             Comment        Usage  \n",
       "0                  like this if you are a tribe fan   PrivateTest  \n",
       "1                                      you re idiot   PrivateTest  \n",
       "2   i am a woman babs and the only war on women i...  PrivateTest  \n",
       "3   wow you benefitted so many wins this year fro...  PrivateTest  \n",
       "4   haha green me red you now loser whos winning ...  PrivateTest  \n",
       "5  and god both the difference between a fag and ...  PrivateTest  \n",
       "6   oh go kiss the ass of a goat and you dummycra...  PrivateTest  \n",
       "7                     not a chance kid you re wrong   PrivateTest  \n",
       "8                on some real shit fuck live jasmin   PrivateTest  \n",
       "9   ok but where the hell was it released you all...  PrivateTest  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "LocationTrain=r'/home/dimitris/Data_Mining/Project3/data/train.csv'\n",
    "LocationTestX=r'/home/dimitris/Data_Mining/Project3/data/impermium_verification_set.csv'\n",
    "\n",
    "dfTestX = pd.read_csv(LocationTestX,low_memory=False)\n",
    "dfTrainCSV = pd.read_csv(LocationTrain,low_memory=False)\n",
    "\n",
    "dfTrainCSV.head(20)\n",
    "\n",
    "\n",
    "for c in range(len(dfTrainCSV.Comment)):\n",
    "        \n",
    "    dfTrainCSV.Comment[c] = dfTrainCSV.Comment[c].lower()\n",
    "    dfTrainCSV.Comment[c] = sNormalization(dfTrainCSV.Comment[c])\n",
    "\n",
    "    \n",
    "for c in range(len(dfTestX.Comment)):\n",
    "    \n",
    "    dfTestX.Comment[c] = dfTestX.Comment[c].lower()\n",
    "    dfTestX.Comment[c] = sNormalization(dfTestX.Comment[c])\n",
    "\n",
    "dfTrainCSV.head(20)\n",
    "dfTestX.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Converting the comments into word vectors, with CountVectorizer method and using NaiveBayes algorithm for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions and libraries:\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB                               #For Naive-Bayes classification method.\n",
    "from sklearn.feature_extraction.text import CountVectorizer              #BoW\n",
    "from sklearn import preprocessing                                        #Create numeric categories using the LabelEncoder and fit-trasnsform pipeline\n",
    "\n",
    "#Our metrics we are going to use:\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#This function it's going to convert the words into numbers for our classification method(NB for now).\n",
    "\n",
    "def Word_Vectorizer(X_train, Y_train, X_test, Y_test, method, bigram):\n",
    "    \n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(Y_train)\n",
    "    \n",
    "    #Y_train and Y_test form now and then , will have numeric values instead of strings\n",
    "    #Same for X_train and X_test\n",
    "\n",
    "    Y_train = le.transform(Y_train)\n",
    "    Y_test  = le.transform(Y_test)\n",
    "    \n",
    "    if(bigram==True):\n",
    "        vectorizer = method(ngram_range=(2, 2))\n",
    "    else:\n",
    "        vectorizer = method()\n",
    "        \n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "    X_test  = vectorizer.transform(X_test)\n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "def print_TTS(Y_pred,Y_test):                                            #Function to print all the scores we want \n",
    "\n",
    "    print(\"Accuracy Score: \" ,accuracy_score(Y_test,Y_pred))\n",
    "#     print(\"Precision Score before optimization: \" ,precision_score(Y_test,Y_pred, average='weighted'))\n",
    "#     print(\"Recall Score before optimization: \", recall_score(Y_test,Y_pred, average='weighted'))\n",
    "    print(\"F1 Score: \", f1_score(Y_test,Y_pred, average='weighted') , \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.5230425055928412\n",
      "F1 Score:  0.5230014478328727 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LocationTestX=r'/home/dimitris/Data_Mining/Project3/data/impermium_verification_set.csv'\n",
    "LocationTestY=r'/home/dimitris/Data_Mining/Project3/data/impermium_verification_labels.csv'\n",
    "\n",
    "# dfTestX = pd.read_csv(LocationTestX,low_memory=False)\n",
    "dfTestY = pd.read_csv(LocationTestY,low_memory=False)\n",
    "\n",
    "dfTestX.head(5)\n",
    "dfTestY.head(5)\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = Word_Vectorizer(dfTrainCSV.Comment, dfTrainCSV.Insult, dfTestX.Comment , dfTestY.Insult , CountVectorizer, False)\n",
    "\n",
    "\n",
    "clf_NB = GaussianNB()\n",
    "\n",
    "clf_NB.fit(X_train.toarray() ,Y_train)\n",
    "\n",
    "Y_pred = clf_NB.predict(X_test.toarray())\n",
    "\n",
    "print_TTS(Y_pred,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Optimization of Naive Bayes with: i)lemmatization , ii) removing stop words, iii) using bigrams and iv) using Laplace Smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions and libraries:\n",
    "import nltk\n",
    "# nltk.download()\n",
    "from nltk.stem import WordNetLemmatizer            #For lemmatization\n",
    "from nltk.corpus import stopwords                  #For stop words\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def Optimize_String(string):\n",
    "    \n",
    "    string = list(string.split(\" \"))\n",
    "\n",
    "    for l in range(len(string)):\n",
    "        string[l] = lemmatizer.lemmatize(string[l])\n",
    "    \n",
    "    filtered_sentence = [w for w in string if not w in stop_words]\n",
    "    filtered_sentence = []\n",
    "\n",
    "\n",
    "    for w in string: \n",
    "        if w not in stop_words: \n",
    "            filtered_sentence.append(w) \n",
    "    \n",
    "\n",
    "    prefinal_string=\"\"\n",
    "    for i in filtered_sentence:\n",
    "        prefinal_string = prefinal_string + i +\" \"\n",
    "\n",
    "    return prefinal_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.5946308724832214\n",
      "F1 Score:  0.5376377058576984 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in range(len(dfTrainCSV.Comment)):\n",
    "    \n",
    "    dfTrainCSV.Comment[c] = Optimize_String(dfTrainCSV.Comment[c])\n",
    "    \n",
    "for c in range(len(dfTestX.Comment)):\n",
    "    \n",
    "    dfTestX.Comment[c] = Optimize_String(dfTestX.Comment[c])\n",
    "    \n",
    "dfTrainCSV.head(5)\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = Word_Vectorizer(dfTrainCSV.Comment, dfTrainCSV.Insult, dfTestX.Comment , dfTestY.Insult , CountVectorizer, True)\n",
    "\n",
    "clf_MNB = MultinomialNB(alpha=1)\n",
    "\n",
    "clf_MNB.fit(X_train.toarray() ,Y_train)\n",
    "\n",
    "Y_pred = clf_MNB.predict(X_test.toarray())\n",
    "\n",
    "print_TTS(Y_pred,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Creating a more complex matrix which will include part-of-speech and TF-IDF-based characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Function and libraries:\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer              #tf-idf\n",
    "\n",
    "def MatrixCreation(dfTrainCSV):\n",
    "\n",
    "    noun      = 0\n",
    "    verb      = 0 \n",
    "    adverb    = 0\n",
    "    adjective = 0\n",
    "    total     = 0 \n",
    "\n",
    "    dfTrainCSV = dfTrainCSV.assign(fractionAdverbs=0.0)\n",
    "    dfTrainCSV = dfTrainCSV.assign(fractionVerbs=0.0)\n",
    "    dfTrainCSV = dfTrainCSV.assign(fractionAdjectives=0.0)\n",
    "    dfTrainCSV = dfTrainCSV.assign(fractionNouns=0.0)\n",
    "\n",
    "    for c in range(len(dfTrainCSV.Comment)):\n",
    "\n",
    "        noun      = 0\n",
    "        verb      = 0 \n",
    "        adverb    = 0\n",
    "        adjective = 0\n",
    "        total     = 0\n",
    "\n",
    "        tokens = nltk.word_tokenize(dfTrainCSV.Comment[c])\n",
    "        tags = nltk.pos_tag(tokens)\n",
    "        counts = Counter(tag for word,tag in tags)\n",
    "\n",
    "#         print(counts)\n",
    "#         print(c)\n",
    "#         print(dfTrainCSV.Comment[c])\n",
    "\n",
    "        for key in counts:\n",
    "\n",
    "            if ('NN' == key or 'NNS' == key or 'NNP' == key or 'NNPS' == key):\n",
    "                noun = noun + counts[key]\n",
    "\n",
    "            elif ('VB' == key or 'VBG' == key or 'VBD' == key or 'VBN' == key or 'VBP' == key or 'VBZ' == key):\n",
    "                verb = verb + counts[key]\n",
    "\n",
    "            elif ('RB' == key or 'RBR' == key or 'RBS' == key):\n",
    "                adverb = adverb + counts[key]\n",
    "\n",
    "            elif ('JJ' == key or 'JJR' == key or 'JJS' == key):\n",
    "                adjective = adjective + counts[key]\n",
    "\n",
    "            total = noun + verb + adverb + adjective\n",
    "\n",
    "            if(total!=0):\n",
    "                dfTrainCSV.fractionAdverbs[c]    = float(adverb/total)\n",
    "                dfTrainCSV.fractionVerbs[c]      = float(verb/total)\n",
    "                dfTrainCSV.fractionAdjectives[c] = float(adjective/total)\n",
    "                dfTrainCSV.fractionNouns[c]      = float(noun/total)\n",
    "            else:\n",
    "                dfTrainCSV.fractionAdverbs[c]    = 0\n",
    "                dfTrainCSV.fractionVerbs[c]      = 0\n",
    "                dfTrainCSV.fractionAdjectives[c] = 0\n",
    "                dfTrainCSV.fractionNouns[c]      = 0\n",
    "\n",
    "\n",
    "#     vectorizer = method() \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train = vectorizer.fit_transform(dfTrainCSV.Comment)\n",
    "    dfTrainCSV['TFIDF']=list(X_train)\n",
    "    return dfTrainCSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Insult</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "      <th>fractionAdverbs</th>\n",
       "      <th>fractionVerbs</th>\n",
       "      <th>fractionAdjectives</th>\n",
       "      <th>fractionNouns</th>\n",
       "      <th>TFIDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20120618192155Z</td>\n",
       "      <td>fuck dad</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>(0, 2889)\\t0.8507528147464873\\n  (0, 4651)\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>20120528192215Z</td>\n",
       "      <td>really understand seems mixing apple orange</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>(0, 8073)\\t0.45521807833090183\\n  (0, 812)\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>canadian ha wrong supportive idea nothing full...</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>(0, 6965)\\t0.23547272025343075\\n  (0, 2127)\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>listen dont wanna get married man woman dont ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>(0, 6243)\\t0.194721427931084\\n  (0, 11150)\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>20120619094753Z</td>\n",
       "      <td>2011 giang ta khi sau tranh con</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>(0, 2475)\\t0.32874153023930996\\n  (0, 11715)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>20120620171226Z</td>\n",
       "      <td>sdl ok would hope sign one year contract star...</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>(0, 5105)\\t0.2802801222547316\\n  (0, 8013)\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>20120503012628Z</td>\n",
       "      <td>yeah</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>(0, 12701)\\t1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>shut fuck rest faggot friend burned stake</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>(0, 10851)\\t0.4553958756839454\\n  (0, 1805)\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>20120502173553Z</td>\n",
       "      <td>either fake extremely stupid maybe</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>(0, 7107)\\t0.4058910063879863\\n  (0, 11042)\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>20120620160512Z</td>\n",
       "      <td>idiot understands neither taxation woman heal...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>(0, 5256)\\t0.3568801660485286\\n  (0, 11320)\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Insult             Date                                            Comment  \\\n",
       "0       1  20120618192155Z                                         fuck dad     \n",
       "1       0  20120528192215Z      really understand seems mixing apple orange     \n",
       "2       0              NaN  canadian ha wrong supportive idea nothing full...   \n",
       "3       0              NaN   listen dont wanna get married man woman dont ...   \n",
       "4       0  20120619094753Z                  2011 giang ta khi sau tranh con     \n",
       "5       0  20120620171226Z   sdl ok would hope sign one year contract star...   \n",
       "6       0  20120503012628Z                                             yeah     \n",
       "7       1              NaN        shut fuck rest faggot friend burned stake     \n",
       "8       1  20120502173553Z               either fake extremely stupid maybe     \n",
       "9       1  20120620160512Z   idiot understands neither taxation woman heal...   \n",
       "\n",
       "   fractionAdverbs  fractionVerbs  fractionAdjectives  fractionNouns  \\\n",
       "0         0.000000       0.000000            0.000000       1.000000   \n",
       "1         0.166667       0.333333            0.166667       0.333333   \n",
       "2         0.083333       0.125000            0.333333       0.458333   \n",
       "3         0.000000       0.160000            0.320000       0.520000   \n",
       "4         0.000000       0.000000            0.000000       1.000000   \n",
       "5         0.047619       0.285714            0.190476       0.476190   \n",
       "6         0.000000       0.000000            0.000000       1.000000   \n",
       "7         0.000000       0.285714            0.142857       0.571429   \n",
       "8         0.500000       0.250000            0.250000       0.000000   \n",
       "9         0.000000       0.200000            0.000000       0.800000   \n",
       "\n",
       "                                               TFIDF  \n",
       "0    (0, 2889)\\t0.8507528147464873\\n  (0, 4651)\\t...  \n",
       "1    (0, 8073)\\t0.45521807833090183\\n  (0, 812)\\t...  \n",
       "2    (0, 6965)\\t0.23547272025343075\\n  (0, 2127)\\...  \n",
       "3    (0, 6243)\\t0.194721427931084\\n  (0, 11150)\\t...  \n",
       "4    (0, 2475)\\t0.32874153023930996\\n  (0, 11715)...  \n",
       "5    (0, 5105)\\t0.2802801222547316\\n  (0, 8013)\\t...  \n",
       "6                                    (0, 12701)\\t1.0  \n",
       "7    (0, 10851)\\t0.4553958756839454\\n  (0, 1805)\\...  \n",
       "8    (0, 7107)\\t0.4058910063879863\\n  (0, 11042)\\...  \n",
       "9    (0, 5256)\\t0.3568801660485286\\n  (0, 11320)\\...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTrainCSV = MatrixCreation(dfTrainCSV)\n",
    "dfTestX    = MatrixCreation(dfTestX)\n",
    "dfTrainCSV.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Insult</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Usage</th>\n",
       "      <th>fractionAdverbs</th>\n",
       "      <th>fractionVerbs</th>\n",
       "      <th>fractionAdjectives</th>\n",
       "      <th>fractionNouns</th>\n",
       "      <th>TFIDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20120603163526Z</td>\n",
       "      <td>like tribe fan</td>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>(0, 2037)\\t0.48756888544606264\\n  (0, 5720)\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20120531215447Z</td>\n",
       "      <td>idiot</td>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>(0, 2737)\\t1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20120823164228Z</td>\n",
       "      <td>woman babs war woman see coming jackazzes lik...</td>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>(0, 609)\\t0.22732114366402909\\n  (0, 4661)\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20120826010752Z</td>\n",
       "      <td>wow benefitted many win year bat nice stupid</td>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>(0, 5391)\\t0.24634768618001143\\n  (0, 3826)\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20120602223825Z</td>\n",
       "      <td>haha green red loser winning moron</td>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>(0, 3679)\\t0.3022470562340654\\n  (0, 6139)\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20120603202442Z</td>\n",
       "      <td>god difference fag fart put meat</td>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>(0, 3508)\\t0.45349859672945864\\n  (0, 4478)\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20120603163604Z</td>\n",
       "      <td>oh go kiss goat dummycraps insult veteran eve...</td>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>(0, 747)\\t0.1986763998913567\\n  (0, 2523)\\t0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20120602223902Z</td>\n",
       "      <td>chance kid wrong</td>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>(0, 6203)\\t0.5421281240485503\\n  (0, 3097)\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20120528064125Z</td>\n",
       "      <td>real shit fuck live jasmin</td>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>(0, 2965)\\t0.6499609927768615\\n  (0, 3311)\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20120603071243Z</td>\n",
       "      <td>ok hell wa released copy article pther anyone...</td>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>(0, 5548)\\t0.2007195442429759\\n  (0, 4905)\\t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  Insult             Date  \\\n",
       "0   1     NaN  20120603163526Z   \n",
       "1   2     NaN  20120531215447Z   \n",
       "2   3     NaN  20120823164228Z   \n",
       "3   4     NaN  20120826010752Z   \n",
       "4   5     NaN  20120602223825Z   \n",
       "5   6     NaN  20120603202442Z   \n",
       "6   7     NaN  20120603163604Z   \n",
       "7   8     NaN  20120602223902Z   \n",
       "8   9     NaN  20120528064125Z   \n",
       "9  10     NaN  20120603071243Z   \n",
       "\n",
       "                                             Comment        Usage  \\\n",
       "0                                   like tribe fan    PrivateTest   \n",
       "1                                            idiot    PrivateTest   \n",
       "2   woman babs war woman see coming jackazzes lik...  PrivateTest   \n",
       "3     wow benefitted many win year bat nice stupid    PrivateTest   \n",
       "4               haha green red loser winning moron    PrivateTest   \n",
       "5                 god difference fag fart put meat    PrivateTest   \n",
       "6   oh go kiss goat dummycraps insult veteran eve...  PrivateTest   \n",
       "7                                 chance kid wrong    PrivateTest   \n",
       "8                       real shit fuck live jasmin    PrivateTest   \n",
       "9   ok hell wa released copy article pther anyone...  PrivateTest   \n",
       "\n",
       "   fractionAdverbs  fractionVerbs  fractionAdjectives  fractionNouns  \\\n",
       "0         0.000000       0.000000            0.000000       1.000000   \n",
       "1         0.000000       0.000000            0.000000       1.000000   \n",
       "2         0.000000       0.357143            0.142857       0.500000   \n",
       "3         0.125000       0.250000            0.250000       0.375000   \n",
       "4         0.000000       0.166667            0.333333       0.500000   \n",
       "5         0.000000       0.166667            0.166667       0.666667   \n",
       "6         0.125000       0.250000            0.312500       0.312500   \n",
       "7         0.000000       0.000000            0.333333       0.666667   \n",
       "8         0.000000       0.000000            0.600000       0.400000   \n",
       "9         0.066667       0.200000            0.066667       0.666667   \n",
       "\n",
       "                                               TFIDF  \n",
       "0    (0, 2037)\\t0.48756888544606264\\n  (0, 5720)\\...  \n",
       "1                                     (0, 2737)\\t1.0  \n",
       "2    (0, 609)\\t0.22732114366402909\\n  (0, 4661)\\t...  \n",
       "3    (0, 5391)\\t0.24634768618001143\\n  (0, 3826)\\...  \n",
       "4    (0, 3679)\\t0.3022470562340654\\n  (0, 6139)\\t...  \n",
       "5    (0, 3508)\\t0.45349859672945864\\n  (0, 4478)\\...  \n",
       "6    (0, 747)\\t0.1986763998913567\\n  (0, 2523)\\t0...  \n",
       "7    (0, 6203)\\t0.5421281240485503\\n  (0, 3097)\\t...  \n",
       "8    (0, 2965)\\t0.6499609927768615\\n  (0, 3311)\\t...  \n",
       "9    (0, 5548)\\t0.2007195442429759\\n  (0, 4905)\\t...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTestX.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Below this cell , we are going to take advantage of POS-Tagging and TF-IDF method, in combination with SVM and Random Decision Forest classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "canadian ha wrong supportive idea nothing full proof perfect take chance inadvertently kill son daughter break always regard collateral damage like wartime sorry cheque mail  \n",
      "canadian_JJ ha_NN wrong_JJ supportive_JJ idea_NN nothing_NN full_JJ proof_NN perfect_JJ take_VB chance_NN inadvertently_RB kill_VB son_JJ daughter_NN break_NN always_RB regard_VB collateral_JJ damage_NN like_IN wartime_NN sorry_NN cheque_JJ mail_NN \n"
     ]
    }
   ],
   "source": [
    "#Functions and libraries:\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC , SVC                                  #For SVM \n",
    "from sklearn.ensemble import RandomForestClassifier                      #For Random Decision Forest \n",
    "from sklearn.model_selection import train_test_split, cross_val_score    #To divide our data evenly\n",
    "\n",
    "\n",
    "\n",
    "string = dfTrainCSV.Comment[2]\n",
    "print(string)\n",
    "\n",
    "def POS_Tagging_Convert(string):\n",
    "    \n",
    "    tokens = nltk.word_tokenize(string)\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "#     print(tags)\n",
    "\n",
    "    new_string = \"\"\n",
    "\n",
    "    for tuple in tags:\n",
    "        new_string =new_string + tuple[0] + \"_\" + tuple[1] + \" \"\n",
    "    \n",
    "    return new_string\n",
    "\n",
    "string = POS_Tagging_Convert(string)\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in range(len(dfTrainCSV.Comment)):\n",
    "    dfTrainCSV.Comment[c] = POS_Tagging_Convert(dfTrainCSV.Comment[c])\n",
    "\n",
    "for c in range(len(dfTestX.Comment)):\n",
    "    dfTestX.Comment[c]    = POS_Tagging_Convert(dfTestX.Comment[c])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For SVM, we have:\n",
      "\n",
      "Accuracy Score:  0.6187919463087248\n",
      "F1 Score:  0.5606414564030837 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfTrainCSV.head(10)\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = Word_Vectorizer(dfTrainCSV.Comment, dfTrainCSV.Insult, dfTestX.Comment , dfTestY.Insult , TfidfVectorizer, False )\n",
    "\n",
    "clf_svm = svm.SVC()\n",
    "\n",
    "clf_svm.fit(X_train ,Y_train)\n",
    "\n",
    "Y_pred = clf_svm.predict(X_test)\n",
    "print(\"For SVM, we have:\\n\")\n",
    "print_TTS(Y_pred,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Random Decision Forest, we have:\n",
      "\n",
      "Accuracy Score:  0.5181208053691275\n",
      "F1 Score:  0.3536598247081931 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = Word_Vectorizer(dfTrainCSV.Comment, dfTrainCSV.Insult, dfTestX.Comment , dfTestY.Insult , TfidfVectorizer, False )\n",
    "\n",
    "clf_rf = RandomForestClassifier(max_depth=5, random_state=0)\n",
    "\n",
    "clf_rf.fit(X_train, Y_train)\n",
    "Y_pred = clf_rf.predict(X_test)\n",
    "print(\"For Random Decision Forest, we have:\\n\")\n",
    "print_TTS(Y_pred,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Beat the BenchMark:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### First, we are going to merge train and test samples into one. Then , we are going to find which is the best train sample, by using: Train, Test & Split Method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At start, we are going to merge Insult labels column into our test sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Insult</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Usage</th>\n",
       "      <th>fractionAdverbs</th>\n",
       "      <th>fractionVerbs</th>\n",
       "      <th>fractionAdjectives</th>\n",
       "      <th>fractionNouns</th>\n",
       "      <th>TFIDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>20120603163526Z</td>\n",
       "      <td>like_IN tribe_NN fan_NN</td>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>(0, 2037)\\t0.48756888544606264\\n  (0, 5720)\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>20120531215447Z</td>\n",
       "      <td>idiot_NN</td>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>(0, 2737)\\t1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>20120823164228Z</td>\n",
       "      <td>woman_NN babs_VBZ war_NN woman_NN see_VBP comi...</td>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.500</td>\n",
       "      <td>(0, 609)\\t0.22732114366402909\\n  (0, 4661)\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>20120826010752Z</td>\n",
       "      <td>wow_RB benefitted_VBN many_JJ win_VBP year_NN ...</td>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.375</td>\n",
       "      <td>(0, 5391)\\t0.24634768618001143\\n  (0, 3826)\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>20120602223825Z</td>\n",
       "      <td>haha_NN green_JJ red_JJ loser_NN winning_VBG m...</td>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500</td>\n",
       "      <td>(0, 3679)\\t0.3022470562340654\\n  (0, 6139)\\t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  Insult             Date  \\\n",
       "0   1       0  20120603163526Z   \n",
       "1   2       1  20120531215447Z   \n",
       "2   3       1  20120823164228Z   \n",
       "3   4       1  20120826010752Z   \n",
       "4   5       1  20120602223825Z   \n",
       "\n",
       "                                             Comment        Usage  \\\n",
       "0                           like_IN tribe_NN fan_NN   PrivateTest   \n",
       "1                                          idiot_NN   PrivateTest   \n",
       "2  woman_NN babs_VBZ war_NN woman_NN see_VBP comi...  PrivateTest   \n",
       "3  wow_RB benefitted_VBN many_JJ win_VBP year_NN ...  PrivateTest   \n",
       "4  haha_NN green_JJ red_JJ loser_NN winning_VBG m...  PrivateTest   \n",
       "\n",
       "   fractionAdverbs  fractionVerbs  fractionAdjectives  fractionNouns  \\\n",
       "0            0.000       0.000000            0.000000          1.000   \n",
       "1            0.000       0.000000            0.000000          1.000   \n",
       "2            0.000       0.357143            0.142857          0.500   \n",
       "3            0.125       0.250000            0.250000          0.375   \n",
       "4            0.000       0.166667            0.333333          0.500   \n",
       "\n",
       "                                               TFIDF  \n",
       "0    (0, 2037)\\t0.48756888544606264\\n  (0, 5720)\\...  \n",
       "1                                     (0, 2737)\\t1.0  \n",
       "2    (0, 609)\\t0.22732114366402909\\n  (0, 4661)\\t...  \n",
       "3    (0, 5391)\\t0.24634768618001143\\n  (0, 3826)\\...  \n",
       "4    (0, 3679)\\t0.3022470562340654\\n  (0, 6139)\\t...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTestX[\"Insult\"] = dfTestY.Insult\n",
    "dfTestX.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we are going to merge test and train samples into a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Insult</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "      <th>fractionAdverbs</th>\n",
       "      <th>fractionVerbs</th>\n",
       "      <th>fractionAdjectives</th>\n",
       "      <th>fractionNouns</th>\n",
       "      <th>TFIDF</th>\n",
       "      <th>id</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20120618192155Z</td>\n",
       "      <td>fuck_NN dad_NN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>(0, 2889)\\t0.8507528147464873\\n  (0, 4651)\\t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>20120528192215Z</td>\n",
       "      <td>really_RB understand_JJ seems_VBZ mixing_VBG a...</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>(0, 8073)\\t0.45521807833090183\\n  (0, 812)\\t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>canadian_JJ ha_NN wrong_JJ supportive_JJ idea_...</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>(0, 6965)\\t0.23547272025343075\\n  (0, 2127)\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>listen_JJ dont_NN wan_NN na_TO get_VB married_...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>(0, 6243)\\t0.194721427931084\\n  (0, 11150)\\t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>20120619094753Z</td>\n",
       "      <td>2011_CD giang_NN ta_NN khi_NN sau_NN tranh_NN ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>(0, 2475)\\t0.32874153023930996\\n  (0, 11715)...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>20120620171226Z</td>\n",
       "      <td>sdl_NN ok_NNS would_MD hope_VB sign_NN one_CD ...</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>(0, 5105)\\t0.2802801222547316\\n  (0, 8013)\\t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>20120503012628Z</td>\n",
       "      <td>yeah_NN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>(0, 12701)\\t1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>shut_JJ fuck_NN rest_NN faggot_VBD friend_NN b...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>(0, 10851)\\t0.4553958756839454\\n  (0, 1805)\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>20120502173553Z</td>\n",
       "      <td>either_DT fake_VBP extremely_RB stupid_JJ mayb...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>(0, 7107)\\t0.4058910063879863\\n  (0, 11042)\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>20120620160512Z</td>\n",
       "      <td>idiot_NN understands_VBZ neither_DT taxation_N...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>(0, 5256)\\t0.3568801660485286\\n  (0, 11320)\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Insult             Date                                            Comment  \\\n",
       "0       1  20120618192155Z                                    fuck_NN dad_NN    \n",
       "1       0  20120528192215Z  really_RB understand_JJ seems_VBZ mixing_VBG a...   \n",
       "2       0              NaN  canadian_JJ ha_NN wrong_JJ supportive_JJ idea_...   \n",
       "3       0              NaN  listen_JJ dont_NN wan_NN na_TO get_VB married_...   \n",
       "4       0  20120619094753Z  2011_CD giang_NN ta_NN khi_NN sau_NN tranh_NN ...   \n",
       "5       0  20120620171226Z  sdl_NN ok_NNS would_MD hope_VB sign_NN one_CD ...   \n",
       "6       0  20120503012628Z                                           yeah_NN    \n",
       "7       1              NaN  shut_JJ fuck_NN rest_NN faggot_VBD friend_NN b...   \n",
       "8       1  20120502173553Z  either_DT fake_VBP extremely_RB stupid_JJ mayb...   \n",
       "9       1  20120620160512Z  idiot_NN understands_VBZ neither_DT taxation_N...   \n",
       "\n",
       "   fractionAdverbs  fractionVerbs  fractionAdjectives  fractionNouns  \\\n",
       "0         0.000000       0.000000            0.000000       1.000000   \n",
       "1         0.166667       0.333333            0.166667       0.333333   \n",
       "2         0.083333       0.125000            0.333333       0.458333   \n",
       "3         0.000000       0.160000            0.320000       0.520000   \n",
       "4         0.000000       0.000000            0.000000       1.000000   \n",
       "5         0.047619       0.285714            0.190476       0.476190   \n",
       "6         0.000000       0.000000            0.000000       1.000000   \n",
       "7         0.000000       0.285714            0.142857       0.571429   \n",
       "8         0.500000       0.250000            0.250000       0.000000   \n",
       "9         0.000000       0.200000            0.000000       0.800000   \n",
       "\n",
       "                                               TFIDF  id Usage  \n",
       "0    (0, 2889)\\t0.8507528147464873\\n  (0, 4651)\\t... NaN   NaN  \n",
       "1    (0, 8073)\\t0.45521807833090183\\n  (0, 812)\\t... NaN   NaN  \n",
       "2    (0, 6965)\\t0.23547272025343075\\n  (0, 2127)\\... NaN   NaN  \n",
       "3    (0, 6243)\\t0.194721427931084\\n  (0, 11150)\\t... NaN   NaN  \n",
       "4    (0, 2475)\\t0.32874153023930996\\n  (0, 11715)... NaN   NaN  \n",
       "5    (0, 5105)\\t0.2802801222547316\\n  (0, 8013)\\t... NaN   NaN  \n",
       "6                                    (0, 12701)\\t1.0 NaN   NaN  \n",
       "7    (0, 10851)\\t0.4553958756839454\\n  (0, 1805)\\... NaN   NaN  \n",
       "8    (0, 7107)\\t0.4058910063879863\\n  (0, 11042)\\... NaN   NaN  \n",
       "9    (0, 5256)\\t0.3568801660485286\\n  (0, 11320)\\... NaN   NaN  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumdf = dfTrainCSV.append(dfTestX)\n",
    "sumdf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For SVM, we have:\n",
      "\n",
      "Accuracy Score:  0.7526273241713823\n",
      "F1 Score:  0.7245580339956885 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comment_train , comment_test , label_train , label_test = train_test_split(sumdf.Comment, sumdf.Insult, test_size=0.2)\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = Word_Vectorizer(comment_train , label_train , comment_test , label_test , TfidfVectorizer, False )\n",
    "\n",
    "clf_svm = svm.SVC()\n",
    "\n",
    "clf_svm.fit(X_train ,Y_train)\n",
    "\n",
    "Y_pred = clf_svm.predict(X_test)\n",
    "print(\"For SVM, we have:\\n\")\n",
    "print_TTS(Y_pred,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the things went as we expected to do. We took the best train and test sample and we fed up the Support Vector Machines with these samples. We managed to improved our Accuracy and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
